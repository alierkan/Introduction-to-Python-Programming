{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-1.1. 영어 문장 토큰화하기\n",
    "- 파이썬의 nltk 모듈을 활용하여, 아래 문장들을 토큰화(Tokenization)한 결과를 출력한다\n",
    "    - Sentence 1: “My only regret in life is that I did not drink more wine.”\n",
    "    - Sentence 2: “I drink to make other people more interesting.”\n",
    "    - Sentence 3: “An intelligent man is sometimes forced to be drunk to spend time with his fools.”\n",
    "- **TIP:** nltk 패키지의 word_tokenize 함수를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전처리하고자 하는 문장을 String 변수로 저장한다\n",
    "sent1 = 'My only regret in life is that I did not drink more wine.'\n",
    "sent2 = 'I drink to make other people more interesting.'\n",
    "sent3 = 'An intelligent man is sometimes forced to be drunk to spend time with his fools.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문장을 토큰화한 결과를 출력한다\n",
    "print('Tokens of Sentence 1:')\n",
    "print(nltk.word_tokenize(sent1))\t\t# 문장을 토큰화해 출력한다 \n",
    "print('Tokens of Sentence 2:')\n",
    "print(nltk.word_tokenize(sent2))\t\t# 문장을 토큰화해 출력한다 \n",
    "print('Tokens of Sentence 3:')\n",
    "print(nltk.word_tokenize(sent3))\t\t# 문장을 토큰화해 출력한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-1-2. 영어 문장 품사 태깅(POS tagging)하기\n",
    "- 아래 문장들의 품사를 태깅해 출력한다\n",
    "    - Sentence 1: “My only regret in life is that I did not drink more wine.”\n",
    "    - Sentence 2: “I drink to make other people more interesting.”\n",
    "    - Sentence 3: “An intelligent man is sometimes forced to be drunk to spend time with his fools.”\n",
    "- **TIP:** nltk 패키지의 pos_tag 함수를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "\n",
    "# 전처리하고자 하는 문장을 String 변수로 저장한다\n",
    "sent1 = 'My only regret in life is that I did not drink more wine.'\n",
    "sent2 = 'I drink to make other people more interesting.'\n",
    "sent3 = 'An intelligent man is sometimes forced to be drunk to spend time with his fools.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문장을 토큰화한 후 품사 태깅을 해 결과를 출력한다\n",
    "print('POS tagging Sentence 1:')\n",
    "tokens1 = nltk.word_tokenize(sent1)\t\t\t\t# 문장을 토큰화한다\n",
    "print(nltk.pos_tag(tokens1))\t\t\t\t\t# 토큰화한 문장을 품사 태깅해 출력한다\n",
    "\n",
    "print('POS tagging Sentence 2:')\n",
    "tokens2 = nltk.word_tokenize(sent2)\t\t\t\t# 문장을 토큰화한다\n",
    "print(nltk.pos_tag(tokens2))\t\t\t\t\t# 토큰화한 문장을 품사 태깅해 출력한다\n",
    "\n",
    "print('POS tagging Sentence 3:')\n",
    "tokens3 = nltk.word_tokenize(sent3)\t\t\t\t# 문장을 토큰화한다\n",
    "print(nltk.pos_tag(tokens3))\t\t\t\t\t# 토큰화한 문장을 품사 태깅해 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-1-3. 단어의 기본형 찾기(Lemmatization)\n",
    "- 아래 세 문장을 lemmatize해 그 결과를 출력해 본다\n",
    "    - Sentence 1: “My only regret in life is that I did not drink more wine.”\n",
    "    - Sentence 2: “I drink to make other people more interesting.”\n",
    "    - Sentence 3: “An intelligent man is sometimes forced to be drunk to spend time with his fools.”\n",
    "- **TIP:** nltk 패키지의 wordnet.WordNetLemmatizer 함수를 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# 전처리하고자 하는 문장을 String 변수로 저장한다\n",
    "sent1 = 'My only regret in life is that I did not drink more wine.'\n",
    "sent2 = 'I drink to make other people more interesting.'\n",
    "sent3 = 'An intelligent man is sometimes forced to be drunk to spend time with his fools.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 문장1 lemmatize하기:\n",
    "print('Lemmatizing Sentence 1:')\n",
    "lemma1 = []\t\t\t\t\t\t\t\t\t\t\t# lemmatize된 token들을 담기 위한 리스트를 생성한다\n",
    "tokens1 = nltk.word_tokenize(sent1)\t\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens1:\t\t\t\t\t\t\t\t# for문을 통해 각각의 token을 lemmatize한다\n",
    "\tlemma1.append(lemmatizer.lemmatize(token))\n",
    "print(lemma1)\t\t\t\t\t\t\t\t\t\t# lemmatize된 결과를 출력한다\n",
    "\n",
    "## 문장2 lemmatize하기:\n",
    "print('Lemmatizing Sentence 2:')\n",
    "lemma2 = []\t\t\t\t\t\t\t\t\t\t\t# lemmatize된 token들을 담기 위한 리스트를 생성한다\n",
    "tokens2 = nltk.word_tokenize(sent2)\t\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens2:\t\t\t\t\t\t\t\t# for문을 통해 각각의 token을 lemmatize한다\n",
    "\tlemma2.append(lemmatizer.lemmatize(token))\n",
    "print(lemma2)\t\t\t\t\t\t\t\t\t\t# lemmatize된 결과를 출력한다\n",
    "\n",
    "## 문장3 lemmatize하기:\n",
    "print('Lemmatizing Sentence 3:')\n",
    "lemma3 = []\t\t\t\t\t\t\t\t\t\t\t# lemmatize된 token들을 담기 위한 리스트를 생성한다\n",
    "tokens3 = nltk.word_tokenize(sent3)\t\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens3:\t\t\t\t\t\t\t\t# for문을 통해 각각의 token을 lemmatize한다\n",
    "\tlemma3.append(lemmatizer.lemmatize(token))\n",
    "print(lemma3)\t\t\t\t\t\t\t\t\t\t# lemmatize된 결과를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-1-4. Stopwords 제거하기\n",
    "- 아래 세 문장에서 stopwords를 제거한 결과를 출력해 본다\n",
    "    - Sentence 1: “My only regret in life is that I did not drink more wine.”\n",
    "    - Sentence 2: “I drink to make other people more interesting.”\n",
    "    - Sentence 3: “An intelligent man is sometimes forced to be drunk to spend time with his fools.”\n",
    "- **TIP:**\n",
    "    - nltk.corpus의 stopwords를 불러온다\n",
    "    - nltk 패키지의 stopwords list는 모두 소문자로 이루어져 있음을 유의한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# 전처리하고자 하는 문장을 String 변수로 저장한다\n",
    "sent1 = 'My only regret in life is that I did not drink more wine.'\n",
    "sent2 = 'I drink to make other people more interesting.'\n",
    "sent3 = 'An intelligent man is sometimes forced to be drunk to spend time with his fools.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문장 1의 stopwords 제거\n",
    "print('Removing stopwords in Sentence 1:')\n",
    "result1 = []\t\t\t\t\t\t\t\t\t# stopwords가 제거된 결과를 담기 위한 리스트를 생성한다\n",
    "tokens1 = nltk.word_tokenize(sent1)\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens1:\t\t\t\t\t\t\t# for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다\n",
    "\tif token.lower() not in stopWords:\t\t\t# 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n",
    "\t\tresult1.append(token)\t\t\t\t\t# token을 리스트에 첨부한다\n",
    "print(result1)\t\t\t\t\t\t\t\t\t# 결과를 출력한다\n",
    "\n",
    "# 문장 2의 stopwords 제거\n",
    "print('Removing stopwords in Sentence 2:')\n",
    "result2 = []\t\t\t\t\t\t\t\t\t# stopwords가 제거된 결과를 담기 위한 리스트를 생성한다\n",
    "tokens2 = nltk.word_tokenize(sent2)\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens2:\t\t\t\t\t\t\t# for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다\n",
    "\tif token.lower() not in stopWords:\t\t\t# 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n",
    "\t\tresult2.append(token)\t\t\t\t\t# token을 리스트에 첨부한다\n",
    "print(result2)\t\t\t\t\t\t\t\t\t# 결과를 출력한다\n",
    "\n",
    "# 문장 3의 stopwords 제거\n",
    "print('Removing stopwords in Sentence 3:')\n",
    "result3 = []\t\t\t\t\t\t\t\t\t# stopwords 제거된 결과를 담기 위한 리스트를 생성한다\t\t\n",
    "tokens3 = nltk.word_tokenize(sent3)\t\t\t\t# 문장을 tokenize한다\n",
    "for token in tokens3:\t\t\t\t\t\t\t# for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다\n",
    "\tif token.lower() not in stopWords:\t\t\t# 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n",
    "\t\tresult3.append(token)\t\t\t\t\t# token을 리스트에 첨부한다\n",
    "print(result3)\t\t\t\t\t\t\t\t\t# 결과를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-2-1. 첫 번째 리뷰 전처리하기 (1)\n",
    "- 웹 크롤링 실습에서 수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 그 중 첫 번째 리뷰 텍스트를 토큰화하고 품사 태깅을 해 그 결과를 출력한다\n",
    "- **TIP:**\n",
    "    - readlines() 함수를 활용해 리뷰 데이터를 리스트로 받아온다\n",
    "    - 파일을 열 때 인코딩 설정을 꼭 ‘utf-8’으로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstReview = lines[0]\t\t\t\t\t\t\t\t\t\t\t\t# 첫 번째 리뷰를 가져와 변수에 저장한다\n",
    "tokens = nltk.word_tokenize(firstReview)\t\t\t\t\t\t\t# 첫 번째 리뷰를 tokenize한다\n",
    "tags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t\t# tokenize한 첫 번째 리뷰를 품사 태깅 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Tokens of first review: ')\t\t\t\t\t\t\t\t\t# 첫 번째 리뷰의 토큰은:\n",
    "print(tokens)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 출력한다\n",
    "print('POS tags of first review: ')\t\t\t\t\t\t\t\t\t# 첫 번째 리뷰의 POS tag는:\n",
    "print(tags)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 품사 태그를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-2-2. 첫 번째 리뷰 전처리하기 (2)\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 그 중 첫 번째 리뷰 텍스트에서 stopwords를 제거하고 lemmtization을 수행해 그 결과를 출력한다\n",
    "- **TIP:**\n",
    "    - readlines() 함수를 활용해 리뷰 데이터를 리스트로 받아온다\n",
    "    - 파일을 열 때 인코딩 설정을 꼭 ‘utf-8’으로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstReview = lines[0]\t\t\t\t\t\t\t\t\t\t\t\t# 첫 번째 리뷰를 가져와 변수에 저장한다\n",
    "tokens = nltk.word_tokenize(firstReview)\t\t\t\t\t\t\t# 첫 번째 리뷰를 tokenize한다\n",
    "lemmas = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# lemmatize한 결과를 담기 위한 리스트를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for문을 통해 stopwords 제거와 lemmatization을 수행한다\n",
    "for token in tokens:\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\tif token.lower() not in stopWords:\t\t\t\t\t\t\t\t# 소문자로 변환한 token이 stopwords에 없으면:\n",
    "\t\tlemmas.append(lemmatizer.lemmatize(token))\t\t\t\t\t# lemmatize한 결과를 리스트에 첨부한다\n",
    "\n",
    "print('Lemmas of the first review: ')\t\t\t\t\t\t\t\t# lemmatize한 결과를 출력한다\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-2-3. 전체 리뷰 데이터 전처리하기(1)\n",
    "- 웹 크롤링 실습에서 수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 각각의 리뷰(총 100개)를 전처리해 새로운 텍스트 파일에 저장한다\n",
    "    - 토큰화와 stopwords 제거와 lemmatization을 한 후 저장한다\n",
    "- **TIP:**\n",
    "    - Input 파일과 같은 100줄 짜리 텍스트 파일이 결과로 나오지만 각각의 줄을 보면 lemmatize 된 결과가 나와야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "    \n",
    "reviewProcessedList = []\t\t\t\t\t\t\t\t\t\t\t# 처리된 결과를 담기 위한 리스트를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for문을 통해 각 줄을 전처리한다\n",
    "for line in lines:\n",
    "\treviewProcessed = ''\t\t\t\t\t\t\t\t\t\t\t# 한 줄을 전처리한 결과를 담기 위한 String 변수 생성\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 리뷰를 tokenize한다\n",
    "\tfor token in tokens:\n",
    "\t\tif token.lower() not in stopWords:\t\t\t\t\t\t\t# 소문자로 변환한 token이 stopwords에 없으면:\n",
    "\t\t\treviewProcessed += ' ' + lemmatizer.lemmatize(token)\t# lemmatize한 붙인다\n",
    "\treviewProcessedList.append(reviewProcessed)\t\t\t\t\t\t# 전처리가 끝난 리뷰를 리스트에 첨부한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-2-4-2-3.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-3.txt', 'w', encoding = 'utf-8') as f:\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tfor reviewProcessed in reviewProcessedList:\t\t\t\t\t\t# 각각의 리뷰를 파일에 쓴다\n",
    "\t\tf.write(reviewProcessed + '\\n')\t\t\t\t\t\t\t\t# 각 리뷰를 줄바꿈('\\n')으로 구분한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-2-4. 전체 리뷰 데이터 전처리하기(2)\n",
    "- 웹 크롤링 실습에서 수집하였던 다른 영화 리뷰들을 이전 실습과 같이 전처리해 각각 다른 텍스트 파일에 저장해 보자\n",
    "    - 토큰화와 stopwords 제거와 lemmatization을 한 후 저장한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '올드보이' 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-old_boy.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-old_boy.txt', 'r', encoding = 'utf-8') as f:\t\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "reviewProcessedList = []\t\t\t\t\t\t\t\t\t\t\t# 처리된 결과를 담기 위한 리스트를 생성한다\n",
    "\n",
    "# for문을 통해 각 줄을 전처리한다\n",
    "for line in lines:\n",
    "\treviewProcessed = ''\t\t\t\t\t\t\t\t\t\t\t# 한 줄을 전처리한 결과를 담기 위한 String 변수 생성\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 리뷰를 tokenize한다\n",
    "\tfor token in tokens:\n",
    "\t\tif token.lower() not in stopWords:\t\t\t\t\t\t\t# 소문자로 변환한 token이 stopwords에 없으면:\n",
    "\t\t\treviewProcessed += ' ' + lemmatizer.lemmatize(token)\t# lemmatize한 붙인다\n",
    "\treviewProcessedList.append(reviewProcessed)\t\t\t\t\t\t# 전처리가 끝난 리뷰를 리스트에 첨부한다\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-4-old_boy.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-old_boy.txt', 'w', encoding = 'utf-8') as f:\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tfor reviewProcessed in reviewProcessedList:\t\t\t\t\t\t# 각각의 리뷰를 파일에 쓴다\n",
    "\t\tf.write(reviewProcessed + '\\n')\t\t\t\t\t\t\t\t# 각 리뷰를 줄바꿈('\\n')으로 구분한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '인셉션' 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-inception.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-inception.txt', 'r', encoding = 'utf-8') as f:\t\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "reviewProcessedList = []\t\t\t\t\t\t\t\t\t\t\t# 처리된 결과를 담기 위한 리스트를 생성한다\n",
    "\n",
    "# for문을 통해 각 줄을 전처리한다\n",
    "for line in lines:\n",
    "\treviewProcessed = ''\t\t\t\t\t\t\t\t\t\t\t# 한 줄을 전처리한 결과를 담기 위한 String 변수 생성\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 리뷰를 tokenize한다\n",
    "\tfor token in tokens:\n",
    "\t\tif token.lower() not in stopWords:\t\t\t\t\t\t\t# 소문자로 변환한 token이 stopwords에 없으면:\n",
    "\t\t\treviewProcessed += ' ' + lemmatizer.lemmatize(token)\t# lemmatize한 붙인다\n",
    "\treviewProcessedList.append(reviewProcessed)\t\t\t\t\t\t# 전처리가 끝난 리뷰를 리스트에 첨부한다\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-4-inception.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-inception.txt', 'w', encoding = 'utf-8') as f:\t\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tfor reviewProcessed in reviewProcessedList:\t\t\t\t\t\t# 각각의 리뷰를 파일에 쓴다\n",
    "\t\tf.write(reviewProcessed + '\\n')\t\t\t\t\t\t\t\t# 각 리뷰를 줄바꿈('\\n')으로 구분한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '위플래시' 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk의 WordNetLemmatizer를 불러와 lemmatizer 변수에 저장한다\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-whiplash.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-whiplash.txt', 'r', encoding = 'utf-8') as f:\t\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "reviewProcessedList = []\t\t\t\t\t\t\t\t\t\t\t# 처리된 결과를 담기 위한 리스트를 생성한다\n",
    "\n",
    "# for문을 통해 각 줄을 전처리한다\n",
    "for line in lines:\n",
    "\treviewProcessed = ''\t\t\t\t\t\t\t\t\t\t\t# 한 줄을 전처리한 결과를 담기 위한 String 변수 생성\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 리뷰를 tokenize한다\n",
    "\tfor token in tokens:\n",
    "\t\tif token.lower() not in stopWords:\t\t\t\t\t\t\t# 소문자로 변환한 token이 stopwords에 없으면:\n",
    "\t\t\treviewProcessed += ' ' + lemmatizer.lemmatize(token)\t# lemmatize한 붙인다\n",
    "\treviewProcessedList.append(reviewProcessed)\t\t\t\t\t\t# 전처리가 끝난 리뷰를 리스트에 첨부한다\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-4-whiplash.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-whiplash.txt', 'w', encoding = 'utf-8') as f:\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tfor reviewProcessed in reviewProcessedList:\t\t\t\t\t\t# 각각의 리뷰를 파일에 쓴다\n",
    "\t\tf.write(reviewProcessed + '\\n')\t\t\t\t\t\t\t\t# 각 리뷰를 줄바꿈('\\n')으로 구분한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-3-1. 리뷰에서 많이 등장하는 명사 추출하기\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 가장 많이 사용된 명사 10개를 추출해 출력해 본다\n",
    "- **TIP:**\n",
    "    - 텍스트를 lower() 함수를 사용해 소문자로 변환 후 비교한다\n",
    "    - 파이썬의 collections 패키지의 Counter 함수를 사용해 많이 사용된 명사를 추출한다\n",
    "    - POS tag 가 ‘NN’, ‘NNS’, ‘NNP, ‘NNPS’인 것을 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# collections 패키지로부터 Counter를 불러온다\n",
    "from collections import Counter\n",
    "\n",
    "# 명사를 담기 위한 리스트를 생성한다\n",
    "nounList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# for 문을 통해 각각의 (단어, 태그) 쌍에 접근\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사이면:\n",
    "\t\t\tnounList.append(word.lower())\t\t\t\t\t\t\t# 소문자로 변환한 후 리스트에 첨부한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter(nounList)\t\t\t\t\t\t\t\t\t\t\t# 각 명사의 숫자를 센 결과를 변수에 저장한다\n",
    "print(counts.most_common(10))\t\t\t\t\t\t\t\t\t\t# 가장 흔히 등장하는 10개 명사를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-3-2. 리뷰에서 많이 등장하는 형용사 추출하기\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 가장 많이 사용된 형용사 10개를 추출해 출력해 본다\n",
    "- **TIP:**\n",
    "    - 텍스트를 lower() 함수를 사용해 소문자로 변환 후 비교한다\n",
    "    - 파이썬의 collections 패키지의 Counter 함수를 사용해 많이 사용된 명사를 추출한다\n",
    "    - POS tag 가 ‘JJ’, ‘JJR’, ‘JJS’ 인 것을 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# collections 패키지로부터 Counter를 불러온다\n",
    "from collections import Counter\n",
    "\n",
    "# 형용사를 담기 위한 리스트를 생성한다\n",
    "adjList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# for 문을 통해 각각의 (단어, 태그) 쌍에 접근\t\t\t\t\t\t\t\t\t\t\n",
    "\t\tif tag in ['JJ', 'JJR', 'JS']:\t\t\t\t\t\t\t\t# 만약 태그가 형용사이면:\n",
    "\t\t\tadjList.append(word.lower())\t\t\t\t\t\t\t# 소문자로 변환한 후 리스트에 첨부한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter(adjList)\t\t\t\t\t\t\t\t\t\t\t# 각 형용사의 숫자를 센 결과를 변수에 저장한다\n",
    "print(counts.most_common(10))\t\t\t\t\t\t\t\t\t\t# 가장 흔히 등장하는 10개 형용사를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-3-3. 리뷰에서 많이 등장하는 단어 추출하기\n",
    "- 다른 영화들의 리뷰에서 자주 등장하는 단어를 추출해 본다\n",
    "    - 자신이 관심 있는 품사의 단어들을 자유롭게 추출해 출력해 본다\n",
    "    - 영어 문장 POS 태그를 참고한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '올드보이' 리뷰의 부사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# collections 패키지로부터 Counter를 불러온다\n",
    "from collections import Counter\n",
    "\n",
    "# 부사를 담기 위한 리스트를 생성한다\n",
    "advList = []\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-old_boy.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-old_boy.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# for 문을 통해 각각의 (단어, 태그) 쌍에 접근\n",
    "\t\tif tag in ['RB', 'RBR', 'RBS']:\t\t\t\t\t\t\t\t# 만약 태그가 부사이면:\n",
    "\t\t\tadvList.append(word.lower())\t\t\t\t\t\t\t# 소문자로 변환한 후 리스트에 첨부한다\n",
    "\n",
    "counts = Counter(advList)\t\t\t\t\t\t\t\t\t\t\t# 각 명사의 숫자를 센 결과를 변수에 저장한다\n",
    "print(counts.most_common(10))\t\t\t\t\t\t\t\t\t\t# 가장 흔히 등장하는 10개 부사를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '위플래시' 리뷰의 동사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# collections 패키지로부터 Counter를 불러온다\n",
    "from collections import Counter\n",
    "\n",
    "# 동사를 담기 위한 리스트를 생성한다\n",
    "verbList = []\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-whiplash.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-whiplash.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# for 문을 통해 각각의 (단어, 태그) 쌍에 접근\t\n",
    "\t\tif tag in ['VB', 'VBD', 'VBN', 'VBP', 'VBZ']:\t\t\t\t# 만약 태그가 동사이면:\n",
    "\t\t\tverbList.append(word.lower())\t\t\t\t\t\t\t# 소문자로 변환한 후 리스트에 첨부한다\n",
    "\n",
    "counts = Counter(verbList)\t\t\t\t\t\t\t\t\t\t\t# 각 동사의 숫자를 센 결과를 변수에 저장한다\n",
    "print(counts.most_common(10))\t\t\t\t\t\t\t\t\t\t# 가장 흔히 등장하는 10개 동사를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영화 '인셉션' 리뷰의 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# collections 패키지로부터 Counter를 불러온다\n",
    "from collections import Counter\n",
    "\n",
    "# 명사를 담기 위한 리스트를 생성한다\n",
    "nounList = []\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-inception.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-inception.txt', 'r', encoding = 'utf-8') as f:\t\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# for 문을 통해 각각의 (단어, 태그) 쌍에 접근\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사이면:\n",
    "\t\t\tnounList.append(word.lower())\t\t\t\t\t\t\t# 소문자로 변환한 후 리스트에 첨부한다\n",
    "\n",
    "counts = Counter(nounList)\t\t\t\t\t\t\t\t\t\t\t# 각 명사의 숫자를 센 결과를 변수에 저장한다\n",
    "print(counts.most_common(10))\t\t\t\t\t\t\t\t\t\t# 가장 흔히 등장하는 10개 명사를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-1. 전체 리뷰의 토큰 개수 출력하기\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 전체 리뷰를 토큰화하고 토큰의 개수를 출력해 본다\n",
    "    - 전체 토큰의 개수와 중복되지 않는(unique) 토큰의 개수를 출력해 보자\n",
    "- ** TIPS: **\n",
    "    - 리뷰를 토큰화할 때 lower 함수를 사용해 소문자로 변환한 뒤 토큰화한다\n",
    "    - Nltk의 Text 함수를 사용한다\n",
    "    - 분석의 효율성을 위해 Stopwords를 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다\n",
    "\n",
    "print(len(corpus.tokens))\t\t\t\t\t\t\t\t\t\t\t# 전체 token의 개수를 출력한다\n",
    "print(len(set(corpus.tokens)))\t\t\t\t\t\t\t\t\t\t# unique한 token의 개수를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-2. 토큰의 등장 횟수 시각화하기\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 토큰화한 뒤 가장 많이 등장하는 토큰 50개의 등장 횟수를 그래프로 시각화해본다\n",
    "- **TIPS: **\n",
    "    - nltk의 Text 함수를 사용해 토큰화한 결과를 저장한다\n",
    "    - plot 함수를 사용해 그래프를 시각화해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다\n",
    "\n",
    "corpus.plot(50)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 가장 많이 등장하는 50개 단어의 빈도수를 그래프로 표현해 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-3. 문맥상 유사한 단어 출력하기\n",
    "- 웹 크롤링 실습에서  수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰를 불러와 텍스트 문맥상 ‘Batman’과 ‘Joker’와 유사한(similar) 단어를 출력해 본다\n",
    "- **TIP:**\n",
    "    - similar 함수를 사용해 유사한 단어를 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Batman'과 'Joker'와 유사한 단어를 출력한다\n",
    "print('='*50)\n",
    "print('Similar words with Batman: ')\t\t\t\t\t\n",
    "corpus.similar('Batman')\t\t\t\t\t\t\t\t\t\t\t# 'Batman'과 유사한 단어를 출력한다\n",
    "print('='*50)\n",
    "print('Similar words with Joker: ')RY\n",
    "corpus.similar('Joker')\t\t\t\t\t\t\t\t\t\t\t\t# 'Joker'와 유사한 단어를 출력한다\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-4. 텍스트의 연어(collocation) 출력하기\n",
    "- 실습 1-3-4에서 수집하였던 영화 ‘다크 나이트(The Dark Knight)’ 리뷰 텍스트 내의 연어를 출력해 본다\n",
    "- **TIP:**\n",
    "    - collocations 함수를 사용해 연어를 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-4.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Collocations for reviews of \"The Dark Knight\": ')\n",
    "corpus.collocations()\t\t\t\t\t\t\t\t\t\t\t\t# 다크 나이트 영화 리뷰의 연어를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-5. 다른 리뷰 텍스트 탐색하기\n",
    "- 2-1부터 2-4까지의 실습 내용을 웹 크롤링 실습에서 수집한 다른 영화 리뷰 데이터에 적용해 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '올드보이' 리뷰 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-old_boy.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-old_boy.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다\n",
    "\n",
    "print(len(corpus.tokens))\t\t\t\t\t\t\t\t\t\t\t# 전체 token의 개수를 출력한다\n",
    "print(len(set(corpus.tokens)))\t\t\t\t\t\t\t\t\t\t# unique한 token의 개수를 출력한다\n",
    "corpus.plot(50)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 가장 많이 등장하는 50개 단어의 빈도수를 그래프로 표현해 출력한다\n",
    "print('='*50)\t\n",
    "print('Similar words with old boy: ')\n",
    "corpus.similar('old boy')\t\t\t\t\t\t\t\t\t\t\t# 'old boy'와 유사한 단어를 출력한다\n",
    "print('='*50)\n",
    "print('Collocations for reviews of \"Old boy\": ')\n",
    "corpus.collocations()\t\t\t\t\t\t\t\t\t\t\t\t# 올드 보이의 collocation을 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '인셉션' 리뷰 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-inception.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-inception.txt', 'r', encoding = 'utf-8') as f: \t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다\n",
    "\n",
    "print(len(corpus.tokens))\t\t\t\t\t\t\t\t\t\t\t# 전체 token의 개수를 출력한다\n",
    "print(len(set(corpus.tokens)))\t\t\t\t\t\t\t\t\t\t# unique한 token의 개수를 출력한다\n",
    "corpus.plot(50)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 가장 많이 등장하는 50개 단어의 빈도수를 그래프로 표현해 출력한다\n",
    "print('='*50)\n",
    "print('Similar words with Dream: ')\n",
    "corpus.similar('Dream')\t\t\t\t\t\t\t\t\t\t\t\t# 'Dreams'와 유사한 단어를 출력한다\n",
    "print('='*50)\n",
    "print('Collocations for reviews of \"Inception\": ')\n",
    "corpus.collocations()\t\t\t\t\t\t\t\t\t\t\t\t# 인셉션 영화의 collocation 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '위플래시' 리뷰 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위해 nltk 모듈을 불러온다\n",
    "import nltk\n",
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# open 함수를 통해 'result-2-3-3-5-whiplash.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-3-3-5-whiplash.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\n",
    "\n",
    "tokens = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# token을 담기 위한 리스트를 생성한다\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 줄에 접근한다:\n",
    "\tline = nltk.word_tokenize(line.lower())\t\t\t\t\t\t\t# 각 라인을 소문자로 변환한 후 tokenize한다\n",
    "\tfor word in line:\t\t\t\t\t\t\t\t\t\t\t\t# 라인의 각 token에 for문을 통해 접근한다:\n",
    "\t\tif word not in stopWords:\t\t\t\t\t\t\t\t\t# 만약 token이 stopword가 아니면:\n",
    "\t\t\ttokens.append(word)\t\t\t\t\t\t\t\t\t\t# 리스트에 첨부한다\n",
    "\n",
    "corpus = nltk.Text(tokens)\t\t\t\t\t\t\t\t\t\t\t# Text() 객체를 corpus 변수에 저장한다\n",
    "\n",
    "print(len(corpus.tokens))\t\t\t\t\t\t\t\t\t\t\t# 전체 token의 개수를 출력한다\n",
    "print(len(set(corpus.tokens)))\t\t\t\t\t\t\t\t\t\t# unique한 token의 개수를 출력한다\n",
    "corpus.plot(50)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 가장 많이 등장하는 50개 단어의 빈도수를 그래프로 표현해 출력한다\n",
    "print('='*50)\n",
    "print('Similar words with Drum: ')\n",
    "corpus.similar('drum')\t\t\t\t\t\t\t\t\t\t\t\t# 'drum'과 유사한 단어를 출력한다\n",
    "print('='*50)\n",
    "print('Collocations for reviews of \"Whiplash\": ')\n",
    "corpus.collocations()\t\t\t\t\t\t\t\t\t\t\t\t# 위플래시의 collocation을 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-6. 연관 단어 그래프 만들기(advanced)\n",
    "- 실습 2-4-2-3 에서 결과로 출력한 lemmatize된 영화 다크 나이트의 리뷰를 바탕으로, 연관된(같은 문장에서 등장하는) 명사들 끼리 연결된 그래프를 만들어 출력해 본다\n",
    "    - 구현하기 너무 어려우면 완성된 코드를 참고만 해 연관 단어 그래프 만드는 법을 익힌다\n",
    "- ** TIPS: **\n",
    "    - Unique한 명사들만 추출해 이를 리스트로 만든다\n",
    "    - (총 문장의 개수)를 행의 수로, (unique한 명사의 개수)를 열의 수로 하는 행렬을 만들어 이를 자신의 전치 행렬(Transpose matrix)와 곱한 co-occurrence matrix를 만든다\n",
    "    - 비가중 무향 그래프(Unweighted, undirected graph)를 만들어 명사들 간의 연관 관계를 표현한다\n",
    "    - numpy와 networkx, 그리고 matplotlib 패키지를 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 분석을 위해 필요한 모듈을 불러온다\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique한 명사를 저장하기 위한 셋을 생성한다\n",
    "unqiueNouns = set()\n",
    "# 문장을 담기 위한 리스트를 생성한다\n",
    "sentences = []\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-3.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-3.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tsentences.append(tags)\t\t\t\t\t\t\t\t\t\t\t# 태그를 리스트에 첨부한다\n",
    "\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# tags의 (단어, 태그)쌍을 하나씩 꺼낸다:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tunqiueNouns.add(word)\t\t\t\t\t\t\t\t\t# 셋에 명사를 첨부한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unqiueNouns = list(unqiueNouns)\t\t\t\t\t\t\t\t\t\t# uniqueNouns 셋을 리스트로 변환한다\n",
    "nounIndex = {noun: i for i, noun in enumerate(unqiueNouns)}\t\t\t# enumerate 함수로 각 명사의 index를 지정한다\n",
    "\n",
    "matrix = np.zeros([len(sentences), len(unqiueNouns)])\t\t\t\t# [(문장의 개수) X (unique한 명사의 개수)]크기의 0으로 이루어진 행렬을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\t\t\t\t\t\t\t# enumerate 함수로 각 sentence의 index를 지정한다\n",
    "\tfor word, tag in sentence:\t\t\t\t\t\t\t\t\t\t# 각 문장의 token들의 (word, tag) 쌍에서:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tindex = nounIndex[word]\t\t\t\t\t\t\t\t\t\n",
    "\t\t\tmatrix[i][index] = 1\t\t\t\t\t\t\t\t\t# 행렬의 [i],[index] 번째 원소를 1로 설정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coocurMatrix = matrix.T.dot(matrix)\t\t\t\t\t\t\t\t\t# 행렬의 내적 연산을 통해 co-occurence matrix를 계산한다\n",
    "\n",
    "graph = nx.Graph()\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## co-occurence matrix의 값들로 그래프를 그려간다\n",
    "for i in range(len(unqiueNouns)):\n",
    "\tfor j in range(i+1, len(unqiueNouns)):\n",
    "\t\tif coocurMatrix[i][j] > 30:\n",
    "\t\t\tgraph.add_edge(unqiueNouns[i], unqiueNouns[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 그래프를 시각화한다\n",
    "plt.figure(figsize=(15, 15))\t\t\t\t\t\t\t\t\t\t# 사이즈는 (15,15)로 설정한다\n",
    "layout = nx.random_layout(graph)\t\t\t\t\t\t\t\t\t# random_layout을 적용한다\n",
    "nx.draw(graph, pos=layout, with_labels=True,\t\t\t\t\t\t# 그래프를 그린다\n",
    "        font_size=20, alpha=0.3, node_size=3000)\n",
    "plt.show()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 화면에 띄운다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-4-4-7. 연관 단어 그래프 만들기(advanced) (2)\n",
    "- 앞선 실습에서 전처리한 다른 영화 리뷰들로도 연관 단어 그래프를 만들어 본다\n",
    "    - 출력 결과는 데이터와 파라미터 세팅에 따라 얼마든지 달라질 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '올드보이' 연관 단어 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 분석을 위해 필요한 모듈을 불러온다\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unique한 명사를 저장하기 위한 셋을 생성한다\n",
    "unqiueNouns = set()\n",
    "# 문장을 담기 위한 리스트를 생성한다\n",
    "sentences = []\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-4-old_boy.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-old_boy.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\t\t\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tsentences.append(tags)\t\t\t\t\t\t\t\t\t\t\t# 태그를 리스트에 첨부한다\n",
    "\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# tags의 (단어, 태그)쌍을 하나씩 꺼낸다:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tunqiueNouns.add(word)\t\t\t\t\t\t\t\t\t# 셋에 명사를 첨부한다\n",
    "\n",
    "unqiueNouns = list(unqiueNouns)\t\t\t\t\t\t\t\t\t\t# uniqueNouns 셋을 리스트로 변환한다\n",
    "nounIndex = {noun: i for i, noun in enumerate(unqiueNouns)}\t\t\t# enumerate 함수로 각 명사의 index를 지정한다\n",
    "\n",
    "matrix = np.zeros([len(sentences), len(unqiueNouns)])\t\t\t\t# [(문장의 개수) X (unique한 명사의 개수)]크기의 0으로 이루어진 행렬을 생성한다\n",
    "\n",
    "for i, sentence in enumerate(sentences):\t\t\t\t\t\t\t# enumerate 함수로 각 sentence의 index를 지정한다\n",
    "\tfor word, tag in sentence:\t\t\t\t\t\t\t\t\t\t# 각 문장의 token들의 (word, tag) 쌍에서:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tindex = nounIndex[word]\t\t\t\t\t\t\t\t\t\n",
    "\t\t\tmatrix[i][index] = 1\t\t\t\t\t\t\t\t\t# 행렬의 [i],[index] 번째 원소를 1로 설정한다\n",
    "\n",
    "coocurMatrix = matrix.T.dot(matrix)\t\t\t\t\t\t\t\t\t# 행렬의 내적 연산을 통해 co-occurence matrix를 계산한다\n",
    "\n",
    "graph = nx.Graph()\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 생성한다\n",
    "\n",
    "## co-occurence matrix의 값들로 그래프를 그려간다\n",
    "for i in range(len(unqiueNouns)):\n",
    "\tfor j in range(i+1, len(unqiueNouns)):\n",
    "\t\tif coocurMatrix[i][j] > 30:\n",
    "\t\t\tgraph.add_edge(unqiueNouns[i], unqiueNouns[j])\n",
    "\n",
    "# 그래프를 시각화한다\n",
    "plt.figure(figsize=(15, 15))\t\t\t\t\t\t\t\t\t\t# 사이즈는 (15,15)로 설정한다\n",
    "layout = nx.random_layout(graph)\t\t\t\t\t\t\t\t\t# random_layout을 적용한다\n",
    "nx.draw(graph, pos=layout, with_labels=True,\t\t\t\t\t\t# 그래프를 그린다\n",
    "        font_size=20, alpha=0.3, node_size=3000)\n",
    "plt.show()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 화면에 띄운다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '인셉션' 연관 단어 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 분석을 위해 필요한 모듈을 불러온다\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unique한 명사를 저장하기 위한 셋을 생성한다\n",
    "unqiueNouns = set()\n",
    "# 문장을 담기 위한 리스트를 생성한다\n",
    "sentences = []\n",
    "# open 함수를 통해 'result-2-4-2-4-inception.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-inception.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\n",
    "\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tsentences.append(tags)\t\t\t\t\t\t\t\t\t\t\t# 태그를 리스트에 첨부한다\n",
    "\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# tags의 (단어, 태그)쌍을 하나씩 꺼낸다:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tunqiueNouns.add(word)\t\t\t\t\t\t\t\t\t# 셋에 명사를 첨부한다\n",
    "\n",
    "unqiueNouns = list(unqiueNouns)\t\t\t\t\t\t\t\t\t\t# uniqueNouns 셋을 리스트로 변환한다\n",
    "nounIndex = {noun: i for i, noun in enumerate(unqiueNouns)}\t\t\t# enumerate 함수로 각 명사의 index를 지정한다\n",
    "\n",
    "matrix = np.zeros([len(sentences), len(unqiueNouns)])\t\t\t\t# [(문장의 개수) X (unique한 명사의 개수)]크기의 0으로 이루어진 행렬을 생성한다\n",
    "\n",
    "for i, sentence in enumerate(sentences):\t\t\t\t\t\t\t# enumerate 함수로 각 sentence의 index를 지정한다\n",
    "\tfor word, tag in sentence:\t\t\t\t\t\t\t\t\t\t# 각 문장의 token들의 (word, tag) 쌍에서:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tindex = nounIndex[word]\t\t\t\t\t\t\t\t\t\n",
    "\t\t\tmatrix[i][index] = 1\t\t\t\t\t\t\t\t\t# 행렬의 [i],[index] 번째 원소를 1로 설정한다\n",
    "\n",
    "coocurMatrix = matrix.T.dot(matrix)\t\t\t\t\t\t\t\t\t# 행렬의 내적 연산을 통해 co-occurence matrix를 계산한다\n",
    "\n",
    "graph = nx.Graph()\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 생성한다\n",
    "\n",
    "## co-occurence matrix의 값들로 그래프를 그려간다\n",
    "for i in range(len(unqiueNouns)):\n",
    "\tfor j in range(i+1, len(unqiueNouns)):\n",
    "\t\tif coocurMatrix[i][j] > 30:\n",
    "\t\t\tgraph.add_edge(unqiueNouns[i], unqiueNouns[j])\n",
    "\n",
    "# 그래프를 시각화한다\n",
    "plt.figure(figsize=(15, 15))\t\t\t\t\t\t\t\t\t\t# 사이즈는 (15,15)로 설정한다\n",
    "layout = nx.random_layout(graph)\t\t\t\t\t\t\t\t\t# random_layout을 적용한다\n",
    "nx.draw(graph, pos=layout, with_labels=True,\t\t\t\t\t\t# 그래프를 그린다\n",
    "        font_size=20, alpha=0.3, node_size=3000)\n",
    "plt.show()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 화면에 띄운다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 '위플래시' 연관 단어 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 분석을 위해 필요한 모듈을 불러온다\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unique한 명사를 저장하기 위한 셋을 생성한다\n",
    "unqiueNouns = set()\n",
    "# 문장을 담기 위한 리스트를 생성한다\n",
    "sentences = []\n",
    "\n",
    "# open 함수를 통해 'result-2-4-2-4-whiplash.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-2-4-2-4-whiplash.txt', 'r', encoding = 'utf-8') as f:\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tlines = f.readlines()\t\t\t\t\t\t\t\t\t\t\t\t\t# readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다\n",
    "\t\n",
    "for line in lines:\t\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각 줄에 접근한다:\n",
    "\ttokens = nltk.word_tokenize(line)\t\t\t\t\t\t\t\t# 각 줄을 tokenize한다\n",
    "\ttags = nltk.pos_tag(tokens)\t\t\t\t\t\t\t\t\t\t# tokenize한 결과를 품사 태깅한다\n",
    "\tsentences.append(tags)\t\t\t\t\t\t\t\t\t\t\t# 태그를 리스트에 첨부한다\n",
    "\n",
    "\tfor word, tag in tags:\t\t\t\t\t\t\t\t\t\t\t# tags의 (단어, 태그)쌍을 하나씩 꺼낸다:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tunqiueNouns.add(word)\t\t\t\t\t\t\t\t\t# 셋에 명사를 첨부한다\n",
    "\n",
    "unqiueNouns = list(unqiueNouns)\t\t\t\t\t\t\t\t\t\t# uniqueNouns 셋을 리스트로 변환한다\n",
    "nounIndex = {noun: i for i, noun in enumerate(unqiueNouns)}\t\t\t# enumerate 함수로 각 명사의 index를 지정한다\n",
    "\n",
    "matrix = np.zeros([len(sentences), len(unqiueNouns)])\t\t\t\t# [(문장의 개수) X (unique한 명사의 개수)]크기의 0으로 이루어진 행렬을 생성한다\n",
    "\n",
    "for i, sentence in enumerate(sentences):\t\t\t\t\t\t\t# enumerate 함수로 각 sentence의 index를 지정한다\n",
    "\tfor word, tag in sentence:\t\t\t\t\t\t\t\t\t\t# 각 문장의 token들의 (word, tag) 쌍에서:\n",
    "\t\tif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\t\t\t\t\t\t# 만약 태그가 명사면:\n",
    "\t\t\tindex = nounIndex[word]\t\t\t\t\t\t\t\t\t\n",
    "\t\t\tmatrix[i][index] = 1\t\t\t\t\t\t\t\t\t# 행렬의 [i],[index] 번째 원소를 1로 설정한다\n",
    "\n",
    "coocurMatrix = matrix.T.dot(matrix)\t\t\t\t\t\t\t\t\t# 행렬의 내적 연산을 통해 co-occurence matrix를 계산한다\n",
    "\n",
    "graph = nx.Graph()\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 생성한다\n",
    "\n",
    "## co-occurence matrix의 값들로 그래프를 그려간다\n",
    "for i in range(len(unqiueNouns)):\n",
    "\tfor j in range(i+1, len(unqiueNouns)):\n",
    "\t\tif coocurMatrix[i][j] > 30:\n",
    "\t\t\tgraph.add_edge(unqiueNouns[i], unqiueNouns[j])\n",
    "\n",
    "# 그래프를 시각화한다\n",
    "plt.figure(figsize=(15, 15))\t\t\t\t\t\t\t\t\t\t# 사이즈는 (15,15)로 설정한다\n",
    "layout = nx.random_layout(graph)\t\t\t\t\t\t\t\t\t# random_layout을 적용한다\n",
    "nx.draw(graph, pos=layout, with_labels=True,\t\t\t\t\t\t# 그래프를 그린다\n",
    "        font_size=20, alpha=0.3, node_size=3000)\n",
    "plt.show()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 그래프를 화면에 띄운다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
