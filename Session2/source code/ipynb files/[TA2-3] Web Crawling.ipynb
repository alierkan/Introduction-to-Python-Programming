{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-1-1. 단어의 검색 결과 출력하기\n",
    "- 다음 어학사전(http://dic.daum.net/index.do?dic=all)에 ‘curiosity’ 단어를 검색하였을 때 출력 되는 화면에서 가장 상단의 결과를 출력한다\n",
    "- 영어 단어의 의미(본 예제에서는 '1. 호기심 2. 큐리오시티')를 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 검색하고 싶은 단어 입력하기\n",
    "word = 'curiosity'\n",
    "\n",
    "# 불러오려는 url 입력하기 \n",
    "url = 'http://dic.daum.net/search.do?q=' + word \t\t\t\t\t\t# 디폴트 url에 string 타입의 word 변수를 합쳐서 url 변수 생성\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# find 함수를 통해 찾고자 하는 단어와 단어의 뜻이 있는 HTML 태그를 찾음\n",
    "box = web_page.find('div', {'class': 'search_cleanword'}).find('span', {'class': 'txt_emph1'}) \t# 'curiosity' 텍스트가 있는 곳\n",
    "box2 = web_page.find('ul', {'class': 'list_search'})\t\t\t\t\t\t\t\t\t\t\t# 단어의 뜻이 있는 곳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'box' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b182a7dfbc2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'box' is not defined"
     ]
    }
   ],
   "source": [
    "print(box.get_text())\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "print(box2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-1-2. 여러 단어의 검색 결과 출력하기\n",
    "- 다음 어학사전(http://dic.daum.net/index.do?dic=all)에 ‘curiosity’ , ‘killed’, ‘the’, ‘cat’ 4개 단어를 검색하였을 때 출력 되는 화면에서 가장 상단의 결과를 출력한다\n",
    "- 각 영어 단어의 의미를 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curiosity\n",
      "\n",
      "1.호기심\n",
      "2.큐리오시티\n",
      "\n",
      "killed\n",
      "\n",
      "1.죽음\n",
      "2.사망\n",
      "3.살해되다\n",
      "4.목숨을 잃다\n",
      "\n",
      "the\n",
      "\n",
      "1.그\n",
      "2.그럴수록\n",
      "3.더욱더\n",
      "\n",
      "cat\n",
      "\n",
      "1.고양이\n",
      "2.고양이과 동물\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 검색하고 싶은 단어 입력하기\n",
    "words = ['curiosity', 'killed', 'the', 'cat']\t\t\t# 단어를 여러 개 검색할 것이므로 복수의 단어를 리스트에 저장한다\n",
    "\n",
    "# for문을 통해 각각의 단어와 그 뜻을 출력한다\n",
    "for word in words:\t\t\t\t\t\t\t\t\t\t# 단어의 리스트에서 각각의 단어를 꺼냄\n",
    "\turl = 'http://dic.daum.net/search.do?q=' + word\t\t# 불러오려는 url 입력하기 \n",
    "\tweb = urlopen(url)\t\t\t\t\t\t\t\t\t# urlopen 함수를 통해 web 변수를 생성\t\t\t\t\t\t\t\n",
    "\tweb_page = BeautifulSoup(web, 'html.parser')\t\t# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "\t# find 함수를 통해 찾고자 하는 단어와 단어의 뜻이 있는 HTML 태그를 찾음\n",
    "\tbox = web_page.find('div', {'class': 'search_cleanword'}).find('span',{'class': 'txt_emph1'})\t# 단어가 있는 곳\n",
    "\tbox2 = web_page.findAll('ul', {'class': 'list_search'})[0]\t\t\t\t\t\t\t\t\t\t# 단어의 뜻이 있는 곳\n",
    "\t# get_text 함수로 텍스트를 추출하고 이를 출력한다\n",
    "\tprint(box.get_text())\n",
    "\tprint(box2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-1-3. 여러 단어의 검색 결과 저장하기\n",
    "- 다음 어학사전(http://dic.daum.net/index.do?dic=all)에 ‘curiosity’ , ‘killed’, ‘the’, ‘cat’ 4개 단어를 검색하였을 때 출력 되는 화면에서 가장 상단의 결과를 텍스트 파일에 저장한다\n",
    "- 각 영어 단어의 의미를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 검색하고 싶은 단어 입력하기\n",
    "words = ['curiosity', 'killed', 'the', 'cat']\t\t\t\t\t# 단어를 여러 개 검색할 것이므로 복수의 단어를 리스트에 저장한다\n",
    "\n",
    "# open 함수를 통해 'result-1-1-3.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-1-3.txt', 'w', encoding = 'utf-8') as f:\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\tfor word in words:\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 단어와 그 뜻을 저장한다\n",
    "\t\turl = 'http://dic.daum.net/search.do?q=' + word \t\t# 불러오려는 url 입력하기 \n",
    "\t\tweb = urlopen(url)\t\t\t\t\t\t\t\t\t\t# urlopen 함수를 통해 web 변수를 생성\t\n",
    "\t\tweb_page = BeautifulSoup(web, 'html.parser')\t\t\t# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "\t\t# find 함수를 통해 찾고자 하는 단어와 단어의 뜻이 있는 HTML 태그를 찾음\n",
    "\t\tbox = web_page.find('div', {'class': 'search_cleanword'}).find('span',{'class': 'txt_emph1'})\t# 단어가 있는 곳\n",
    "\t\tbox2 = web_page.find('ul', {'class': 'list_search'})\t\t\t\t\t\t\t\t\t\t\t# 단어의 뜻이 있는 곳\n",
    "\t\t# get_text 함수로 텍스트를 추출하고 이를 저장한다\n",
    "\t\t# write 함수로 파일에 내용을 쓴다\n",
    "\t\tf.write(box.get_text() + '\\n')\t\t\t\t\n",
    "\t\tf.write(box2.get_text() + '\\n')\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-1-4. 여러 단어를 불러와 검색 결과 저장하기\n",
    "- 다음 어학사전(http://dic.daum.net/index.do?dic=all)에 텍스트 파일(data.txt)에 있는 각 단어를 검색하였을 때 출력 되는 화면에서 가장 상단의 결과를 텍스트 파일에 저장한다\n",
    "- 각 영어 단어의 의미를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'data-1-1-4.txt' 파일을 열고 그 내용을 가져온다\n",
    "with open('data-1-1-4.txt', 'r', encoding = 'utf-8') as f:\t\t\t# 읽기 형식('r')로 지정하고 인코딩은 'utf-8'으로 설정한다\n",
    "\twords = f.readlines()\t\t\t\t\t\t\t\t\t\t\t# readlines 함수를 통해 전체 텍스트 파일의 내용을 리스트로 가져온다 (줄로 구분)\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-1-1-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-1-4.txt', 'w', encoding = 'utf-8') as f:\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다 \n",
    "\tfor word in words:\t\t\t\t\t\t\t\t\t\t\t\t# for문을 통해 각각의 단어와 그 뜻을 저장한다\n",
    "\t\turl = 'http://dic.daum.net/search.do?q=' + word \t\t\t# 불러오려는 url 입력하기\n",
    "\t\tweb = urlopen(url)\t\t\t\t\t\t\t\t\t\t\t# urlopen 함수를 통해 web 변수를 생성\t\n",
    "\t\tweb_page = BeautifulSoup(web, 'html.parser')\t\t\t\t# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "\t\tbox = web_page.find('div', {'class': 'search_cleanword'}).find('span',{'class': 'txt_emph1'})\t\t# 단어가 있는 곳\n",
    "\t\tbox2 = web_page.find('ul', {'class': 'list_search'})\t\t\t\t\t\t\t\t\t\t\t\t# 단어의 뜻이 있는 곳\n",
    "\t\t# get_text 함수로 텍스트를 추출하고 이를 저장한다\n",
    "\t\t# write 함수로 파일에 내용을 쓴다\t\t\n",
    "\t\tf.write(box.get_text() + '\\n')\n",
    "\t\tf.write(box2.get_text() + '\\n')\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-2-1. 비트코인 관련 데이터 출력하기\n",
    "- 코인마켓캡(https://coinmarketcap.com/) 메인 페이지에서 가장 상단에 있는 비트코인의 화폐 이름(Name)과 시가총액(Market Cap), 개당 가격(Price), 그리고 24시간 볼륨(Volume)을 출력해 본다\n",
    "- 한 줄에 하나씩 출력되도록 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기 - 코인마켓캡 닷컴\n",
    "url = 'https://coinmarketcap.com/'\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# 비트코인과 관련된 데이터가 있는 곳을 찾아 bit 변수에 저장한다\n",
    "table = web_page.find('table', {'id': 'currencies'})\t# 1위부터 100위까지의 cryptocurrency 데이터가 있는 테이블을 찾는다\n",
    "bit = table.find('tr', {'id': 'id-bitcoin'})\t\t\t# 비트코인 데이터가 행(첫째 행)의 데이터를 찾는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 비트코인 관련된 데이터를 출력한다\n",
    "coinName = bit.find('td', {'class': 'no-wrap currency-name'})\t\t\t\n",
    "print('Coin Name: ', coinName.get_text().strip())\t\t\t\t\t\t\t# 코인의 이름('Bitcoin')을 출력한다\n",
    "marketCap = bit.find('td', {'class': 'no-wrap market-cap text-right'})\n",
    "print('Market Cap: ', marketCap.get_text().strip())\t\t\t\t\t\t\t# 코인의 시가총액(Market cap)을 출력한다\n",
    "price = bit.find('a', {'class': 'price'})\t\t\t\t\t\t\t\t\t\n",
    "print('Price: ', price.get_text().strip())\t\t\t\t\t\t\t\t\t# 코인의 가격(price)를 출력한다\n",
    "volume = bit.find('a', {'class': 'volume'})\n",
    "print('Volume: ', volume.get_text().strip())\t\t\t\t\t\t\t\t# 코인의 24시간 볼륨(volume)을 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-2-2. 상위 100개 코인 관련 데이터 저장하기\n",
    "- 코인마켓캡(https://coinmarketcap.com/) 메인 페이지에 있는 상위 100개 코인의 화폐 이름(Name)과 시가총액(Market Cap), 개당 가격(Price), 그리고 24시간 볼륨(Volume)을 텍스트 파일로 저장한다\n",
    "- 한 줄에 코인 하나의 데이터를 저장하고, 각 행에서는 탭으로 구분한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기 - 코인마켓캡 닷컴\n",
    "url = 'https://coinmarketcap.com/'\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# 코인과 관련된 데이터가 있는 곳을 찾는다\n",
    "table = web_page.find('table', {'id': 'currencies'})\t\t\t\t\t\t\t# 1위부터 100위까지의 cryptocurrency 데이터가 있는 테이블을 찾는다\n",
    "coinNames = table.findAll('td', {'class': 'no-wrap currency-name'})\t\t\t\t# 코인의 이름(name)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "marketCaps = table.findAll('td', {'class': 'no-wrap market-cap text-right'})\t# 코인의 시가총액(market cap)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "prices = table.findAll('a', {'class': 'price'})\t\t\t\t\t\t\t\t\t# 코인의 가격(price)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "volumes = table.findAll('a', {'class': 'volume'})\t\t\t\t\t\t\t\t# 코인의 24시간 볼륨(volume)이 있는 곳들을 찾아 리스트로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-1-2-2.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-2-2.txt', 'w', encoding = 'utf-8') as f:\t\t\t\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\t# for문을 통해 시가총액 1위부터 100위까지의 코인의 정보를 저장한다\n",
    "\tfor i in range(100):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 리스트 인덱스 접근을 위해 0부터 100까지 iterate 한다\n",
    "\t\t# 하나의 코인에 대한 정보를 row에 입력한다\n",
    "\t\t# 중간중간에 '\\t'를 입력하여 각각의 데이터를 구분한다\n",
    "\t\trow = coinNames[i].get_text().strip() + '\\t' + marketCaps[i].get_text().strip() + '\\t' \\\n",
    "\t\t\t+ prices[i].get_text().strip() + '\\t' + volumes[i].get_text().strip() + '\\n'\n",
    "\t\tf.write(row)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# row를 파일에 쓴다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-2-3. 특정 조건을 만족하는 코인 관련 데이터 저장하기\n",
    "- 상위 100개의 코인 중 24시간 볼륨(Volume)이 $1,000,000 이상이고 개당 가격이 $50 이상인 코인들의 데이터만 저장한다\n",
    "- 한 줄에 코인 하나의 데이터를 저장하고, 각 행에서는 탭으로 구분한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기 - 코인마켓캡 닷컴\n",
    "url = 'https://coinmarketcap.com/'\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# 코인과 관련된 데이터가 있는 곳을 찾는다\n",
    "table = web_page.find('table', {'id': 'currencies'})\t\t\t\t\t\t\t# 1위부터 100위까지의 cryptocurrency 데이터가 있는 테이블을 찾는다\n",
    "coinNames = table.findAll('td', {'class': 'no-wrap currency-name'})\t\t\t\t# 코인의 이름(name)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "marketCaps = table.findAll('td', {'class': 'no-wrap market-cap text-right'})\t# 코인의 시가총액(market cap)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "prices = table.findAll('a', {'class': 'price'})\t\t\t\t\t\t\t\t\t# 코인의 가격(price)이 있는 곳들을 찾아 리스트로 저장한다\n",
    "volumes = table.findAll('a', {'class': 'volume'})\t\t\t\t\t\t\t\t# 코인의 24시간 볼륨(volume)이 있는 곳들을 찾아 리스트로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-1-2-3.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-2-3.txt', 'w') as f:\t\t\t\t\t\t\t\t\t\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "# for문을 통해 시가총액 1위부터 100위까지의 코인의 데이터에 접근한다\n",
    "\tfor i in range(100):\n",
    "\t\tprice = float(prices[i].get_text().strip().replace('$','').replace(',',''))\t\t# 코인의 가격을 가져온다(float 타입으로)\n",
    "\t\tvolume = float(volumes[i].get_text().strip().replace('$','').replace(',',''))\t# 코인의 볼륨을 가져온다(float 타입으로)\n",
    "\t\t# if문을 통해 저장 조건을 준다\n",
    "\t\tif volume >= 1000000 and price >= 50:\t\t\t\t\t\t\t\t\t\t\t# 볼륨이 1,000,000 이상이고 가격이 50 이상일 경우:\n",
    "\t\t\t# 하나의 코인에 대한 정보를 row에 입력한다\n",
    "\t\t\t# 중간중간에 '\\t'를 입력하여 각각의 데이터를 구분한다\n",
    "\t\t\trow = coinNames[i].get_text().strip() + '\\t' + marketCaps[i].get_text().strip() + '\\t' \\\n",
    "\t\t\t\t+ prices[i].get_text().strip() + '\\t' + volumes[i].get_text().strip() + '\\n'\n",
    "\t\t\tf.write(row)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# row를 파일에 쓴다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-3-1. '펄프 픽션' 영화 정보 출력하기\n",
    "- IMDb 사이트에서 영화 펄프 픽션 페이지(http://www.imdb.com/title/tt0110912/?ref_=nv_sr_1)에서 펄프 픽션 영화의 제목(title)과 감독(director), 그리고 출연 배우(cast)를 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기 (IMDb - 펄프 픽션)\n",
    "url = 'http://www.imdb.com/title/tt0110912/?ref_=nv_sr_1'\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 영화 제목을 출력한다\n",
    "titleBar = web_page.find('div', {'class': 'titleBar'})\t\t# 타이틀(영화 제목)이 있는 바(bar)를 찾는다\n",
    "title = titleBar.find('h1', {'itemprop': 'name'})\t\t\t# 타이틀바에서 영화 제목이 있는 곳을 찾는다\n",
    "print('Movie: ' , title.get_text())\t\t\t\t\t\t\t# get_text 함수로 영화 제목을 출력한다\n",
    "\n",
    "# 감독 이름을 출력한다\n",
    "director = web_page.find('span', {'itemprop': 'director'})\t# 감독 이름이 있는 곳을 찾는다\n",
    "print('Director: ', director.get_text().strip())\t\t\t# get_text 함수로 감독 이름을 출력한다\n",
    "\n",
    "# 출연 배우 이름(들)을 출력한다\n",
    "table = web_page.find('table', {'class': 'cast_list'})\t\t# 캐스팅 리스트가 있는 테이블을 찾는다\n",
    "find = table.findAll('span', {'class': 'itemprop', 'itemprop': 'name'})\t\t# 배우 이름이 있는 곳들을 찾아 리스트에 저장한다\n",
    "print('Cast: ')\n",
    "# for문을 통해 각 배우의 이름을 출력한다\n",
    "for i in find:\n",
    "\tprint(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-3-2. 여러 영화 정보 저장하기\n",
    "- 영화 펄프 픽션(Pulp Fiction), 포레스트 검프(Forrest Gump), 그리고 다크 나이트(The Dark Knight)의 제목(title)과 감독 이름(director)를 텍스트 파일에 저장한다\n",
    "- 영화 하나당 한 줄씩 출력되도록 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기 - 세 개의 url이 있으므로 이를 리스트에 저장한다\n",
    "urls = ['http://www.imdb.com/title/tt0110912/?ref_=nv_sr_1', \\\n",
    "\t\t'http://www.imdb.com/title/tt0109830/?ref_=tt_rec_tt', \\\n",
    "\t\t'http://www.imdb.com/title/tt0468569/?ref_=nv_sr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 각 영화의 정보를 담기 위한 리스트를 생성\n",
    "\n",
    "# 각각의 url에 접속하면서 영화 정보를 가져온다\n",
    "for url in urls:\n",
    "\tweb = urlopen(url)\t\t\t\t\t\t\t\t\t\t\t# urlopen 함수를 통해 web 변수를 생성\n",
    "\tweb_page = BeautifulSoup(web, 'html.parser')\t\t\t\t# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "\ttitleBar = web_page.find('div', {'class': 'titleBar'})\t\t# 타이틀(영화 제목)이 있는 바(bar)를 찾는다\n",
    "\ttitle = titleBar.find('h1', {'itemprop': 'name'})\t\t\t# 타이틀바에서 영화 제목이 있는 곳을 찾는다\n",
    "\tdirector = web_page.find('span', {'itemprop': 'director'})\t# 감독 이름이 있는 곳을 찾는다\n",
    "\n",
    "\t# info 리스트에 각 영화의 정보를 저장한다\n",
    "\tinfo.append(title.get_text().strip() + '\\t' + director.get_text().strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-1-3-2.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-3-2.txt', 'w', encoding = 'utf-8') as f:\t# 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "\t# info 리스트의 각 요소의 내용을 파일에 쓴다\n",
    "\tfor i in info:\n",
    "\t\tf.write(i + '\\n')\t\t\t\t\t\t\t\t\t\t# 각 정보는 새로운 줄('\\n')으로 구분한다\n",
    "\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t# 파일을 닫는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-3-3. 영화 리뷰 출력하기\n",
    "- 영화 다크 나이트(The Dark Knight)의 리뷰 페이지에 들어가 첫 번째 리뷰 내용을 출력해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오려는 url 입력하기\n",
    "url = 'http://www.imdb.com/title/tt0468569/reviews?start=0'\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "source = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# 리뷰 데이터가 있는 테이블을 찾는다\n",
    "table = source.find(\"div\", {\"id\": \"tn15content\"})\n",
    "\n",
    "# 테이블 내의 모든 paragraph('p')를 찾아 리스트에 저장한다\n",
    "pars = table.findAll('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각각의 paragraph의 내용을 출력한다\n",
    "for par in pars:\n",
    "\t# 리뷰일 경우에만 출력하기 위해 if 조건문을 활용한다\n",
    "    if ('This review may contain spoilers' not in par) and ('Add another review' not in par):\n",
    "        print(par.get_text().replace('\\n', ' ').replace('\\r', ' '))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2-3-3-4. 영화 리뷰 저장하기\n",
    "- 영화 다크 나이트(The Dark Knight)의 첫 번째 부터 10번째 페이지까지의 리뷰를 텍스트 파일에 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 시작 URL을 입력한다\n",
    "baseURL = 'http://www.imdb.com/title/tt0468569/reviews?start='\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성('0'을 더해 첫 번째 리뷰부터 시작함을 알려준다)\n",
    "web = urlopen(baseURL + '0')\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "source = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "# 리뷰 데이터가 있는 테이블을 찾는다\n",
    "table = source.find('div', {'id': 'tn15content'})\n",
    "\n",
    "page_no = 10                    # 출력하고자 하는 총 페이지 개수를 입력\n",
    "reviewList = []                 # 리뷰 내용을 담고자 하는 리스트를 생성\n",
    "status = True                   # while문을 탈출하기 위한 조건을 설정하는 status 변수 생성\n",
    "i = 0                           # 현재 크롤링한 리뷰 수를 기록하기 위한 i 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# while문을 통해 10 번째 페이지까지의 리뷰를 크롤링한다\n",
    "while status:\n",
    "    url = baseURL + str(i)      # 시작 URL에 i를 더해 크롤링하고자 하는 페이지를 입력한다\n",
    "    web = urlopen(url)          # urlopen 함수를 통해 web 변수를 생성 \n",
    "    source = BeautifulSoup(web, 'html.parser')          # BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "    table = source.find('div', {'id': 'tn15content'})   # 리뷰 데이터가 있는 테이블을 찾는다\n",
    "    \n",
    "    pars = table.findAll('p')                           # 테이블 내의 모든 paragraph('p')를 찾아 리스트에 저장한다   \n",
    "\n",
    "    # 각각의 paragraph의 내용을 저장한다\n",
    "    for par in pars:\n",
    "        text = par.get_text().replace('\\n', ' ').replace('\\r', ' ')\n",
    "        # 리뷰일 경우에만 저장하기 위해 if 조건문을 활용한다\n",
    "        if ('This review may contain spoilers' not in text) and ('Add another review' not in text):\n",
    "            reviewList.append(text)\n",
    "    i += 10                                         # 하나의 페이지를 크롤링할때마다 i를 10씩 증가시킨다\n",
    "    if i >= page_no * 10:                           # 만약 현재 크롤링한 페이지 개수가 10을 넘어가면:\n",
    "        status = False                              # while문을 탈출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open 함수를 통해 'result-1-3-4.txt' 파일을 열고 이를 f로 지정한다\n",
    "with open('result-1-3-4.txt', 'w', encoding = 'utf-8') as f:        # 쓰기 형식('w')로 지정하고 인코딩은 'utf-8'로 설정한다\n",
    "    for review in reviewList:                                       # for문을 통해 reviewList의 각각의 리뷰를 파일에 쓴다\n",
    "        f.write(review + '\\n')\n",
    "    f.close()                                                       # 파일을 닫는다"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
